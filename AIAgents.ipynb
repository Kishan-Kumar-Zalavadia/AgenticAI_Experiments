{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuXlaVTrP+7LhL5bnAwt/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kishan-Kumar-Zalavadia/AgenticAI_Experiments/blob/main/AIAgents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install litellm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCjVaLHXoMje",
        "outputId": "68442e90-c9cb-4e93-85d2-2809d2512b11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Requirement already satisfied: litellm in /usr/local/lib/python3.12/dist-packages (1.78.2)',\n",
              " 'Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.13.0)',\n",
              " 'Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm) (8.3.0)',\n",
              " 'Requirement already satisfied: fastuuid>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.13.5)',\n",
              " 'Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.28.1)',\n",
              " 'Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (8.7.0)',\n",
              " 'Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.1.6)',\n",
              " 'Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (4.25.1)',\n",
              " 'Requirement already satisfied: openai>=1.99.5 in /usr/local/lib/python3.12/dist-packages (from litellm) (1.109.1)',\n",
              " 'Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (2.11.10)',\n",
              " 'Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (1.1.1)',\n",
              " 'Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.12.0)',\n",
              " 'Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm) (0.22.1)',\n",
              " 'Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (2.6.1)',\n",
              " 'Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.4.0)',\n",
              " 'Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (25.4.0)',\n",
              " 'Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.8.0)',\n",
              " 'Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (6.7.0)',\n",
              " 'Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (0.3.2)',\n",
              " 'Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.22.0)',\n",
              " 'Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (4.11.0)',\n",
              " 'Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (2025.10.5)',\n",
              " 'Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (1.0.9)',\n",
              " 'Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (3.10)',\n",
              " 'Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)',\n",
              " 'Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)',\n",
              " 'Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)',\n",
              " 'Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.9.1)',\n",
              " 'Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)',\n",
              " 'Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.27.1)',\n",
              " 'Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (1.9.0)',\n",
              " 'Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (0.11.0)',\n",
              " 'Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (1.3.1)',\n",
              " 'Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (4.67.1)',\n",
              " 'Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.99.5->litellm) (4.15.0)',\n",
              " 'Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)',\n",
              " 'Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.33.2)',\n",
              " 'Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)',\n",
              " 'Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)',\n",
              " 'Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.4)',\n",
              " 'Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm) (0.35.3)',\n",
              " 'Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.20.0)',\n",
              " 'Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.3.0)',\n",
              " 'Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)',\n",
              " 'Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)',\n",
              " 'Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.1.10)',\n",
              " 'Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.3)',\n",
              " 'Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.5.0)']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "Lmg1KIvewIEv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programmatic Prompting for Agents"
      ],
      "metadata": {
        "id": "6S7pgJ_3v3xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "from google.colab import userdata\n",
        "\n",
        "response = completion(\n",
        "    model=\"groq/llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert software engineer.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a function to swap keys and values in a dictionary.\"}\n",
        "    ],\n",
        "    max_tokens=128\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRqp1zpZn96_",
        "outputId": "8beccec7-10f9-4045-9ecd-86cdc83d84fc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Swapping Keys and Values in a Dictionary\n",
            "\n",
            "In Python, dictionaries are inherently one-way mappings, meaning each key corresponds to a unique value. However, we can swap keys and values by creating a new dictionary. This approach requires that the original dictionary's values are unique, as dictionaries cannot have duplicate keys.\n",
            "\n",
            "#### Code\n",
            "\n",
            "```python\n",
            "def swap_keys_values(dictionary):\n",
            "    \"\"\"\n",
            "    Swap keys and values in a dictionary.\n",
            "\n",
            "    Args:\n",
            "    dictionary (dict): The input dictionary.\n",
            "\n",
            "    Returns:\n",
            "    dict: A new dictionary with keys and values swapped.\n",
            "\n",
            "    Raises:\n",
            "    ValueError: If the dictionary's values are not unique.\n",
            "   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example-2"
      ],
      "metadata": {
        "id": "OFvuK6kVwTrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(\n",
        "    model=\"groq/llama-3.3-70b-versatile\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert cook.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Give me steps to make chicken curry.\"}\n",
        "    ],\n",
        "    max_tokens=1000\n",
        ")\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_DWPhhqsFM7",
        "outputId": "ea65f730-e7d6-42a3-9942-e36282ace426"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delicious chicken curry. Here's a step-by-step guide to making a flavorful and aromatic chicken curry:\n",
            "\n",
            "**Ingredients:**\n",
            "\n",
            "* 1 pound boneless, skinless chicken breast or thighs, cut into bite-sized pieces\n",
            "* 2 medium onions, chopped\n",
            "* 2 cloves of garlic, minced\n",
            "* 2 medium tomatoes, diced\n",
            "* 1 tablespoon grated fresh ginger\n",
            "* 1 teaspoon ground cumin\n",
            "* 1 teaspoon curry powder\n",
            "* 1/2 teaspoon turmeric\n",
            "* 1/2 teaspoon cayenne pepper (optional)\n",
            "* 1/2 teaspoon salt\n",
            "* 1/4 teaspoon black pepper\n",
            "* 2 tablespoons vegetable oil\n",
            "* 2 tablespoons butter or ghee (optional)\n",
            "* 2 cups chicken broth or water\n",
            "* Fresh cilantro, chopped (for garnish)\n",
            "\n",
            "**Instructions:**\n",
            "\n",
            "1. **Prepare the spice blend:** In a small bowl, combine cumin, curry powder, turmeric, cayenne pepper (if using), salt, and black pepper.\n",
            "2. **Heat oil in a pan:** In a large pan or Dutch oven, heat 1 tablespoon of oil over medium heat.\n",
            "3. **Sauté onions:** Add the chopped onions and sauté until they're translucent and starting to brown, about 5 minutes.\n",
            "4. **Add ginger and garlic:** Add the minced garlic and grated ginger and sauté for another minute, until fragrant.\n",
            "5. **Add the chicken:** Add the chicken pieces and cook until they're browned on all sides, about 5-7 minutes.\n",
            "6. **Add the spice blend:** Add the prepared spice blend and stir to coat the chicken and onions evenly. Cook for 1 minute.\n",
            "7. **Add the tomatoes:** Add the diced tomatoes and cook until they start to break down, about 3-4 minutes.\n",
            "8. **Add broth or water:** Pour in the chicken broth or water and bring the mixture to a simmer.\n",
            "9. **Reduce heat and let it cook:** Reduce the heat to low and let the curry simmer, covered, for 20-25 minutes or until the chicken is cooked through and the sauce has thickened.\n",
            "10. **Add butter or ghee (optional):** If using, stir in the butter or ghee to enrich the sauce and add a creamy texture.\n",
            "11. **Taste and adjust:** Taste the curry and adjust the seasoning as needed.\n",
            "12. **Garnish and serve:** Garnish with chopped cilantro and serve over basmati rice, naan, or with some roti.\n",
            "\n",
            "**Tips and Variations:**\n",
            "\n",
            "* For a thicker sauce, reduce the amount of broth or water or add a little cornstarch or flour to thicken.\n",
            "* For a creamier sauce, add a splash of heavy cream or coconut cream towards the end of cooking.\n",
            "* Experiment with different spice blends or add other aromatics like cinnamon, cardamom, or bay leaves to create unique flavor profiles.\n",
            "* Use bone-in chicken or add some potatoes, carrots, or bell peppers to make the curry more substantial.\n",
            "\n",
            "Enjoy your delicious homemade chicken curry!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s break down the key components:\n",
        "\n",
        "1. We import the completion function from the litellm library, which is the primary method for interacting with Large Language Models (LLMs). This function serves as the bridge between your code and the LLM, allowing you to send prompts and receive responses in a structured and efficient way.\n",
        "\n",
        "    How completion Works:\n",
        "\n",
        "    - Input: You provide a prompt, which is a list of messages that you want the model to process. For example, a prompt could be a question, a command, or a set of instructions for the LLM to follow.\n",
        "    - Output: The completion function returns the model’s response, typically in the form of generated text based on your prompt.\n",
        "\n",
        "2. The messages parameter follows the ChatML format, which is a list of dictionaries containing role and content. The role attribute indicates who is “speaking” in the conversation. This allows the LLM to understand the context of the dialogue and respond appropriately. The roles include:\n",
        "\n",
        "  - “system”: Provides the model with initial instructions, rules, or configuration for how it should behave throughout the session. This message is not part of the “conversation” but sets the ground rules or context (e.g., “You will respond in JSON.”).\n",
        "  - “user”: Represents input from the user. This is where you provide your prompts, questions, or instructions.\n",
        "  - “assistant”: Represents responses from the AI model. You can include this role to provide context for a conversation that has already started or to guide the model by showing sample responses. These messages are interpreted as what the “model” said in the passt.\n",
        "\n",
        "3. We specify the model using the provider/model format (e.g., “openai/gpt-4o”)\n",
        "\n",
        "4. The response contains the generated text in choices[0].message.content. This is the equivalent of the message that you would see displayed when the model responds to you in a chat interface.\n",
        "\n"
      ],
      "metadata": {
        "id": "ea5X7k0vvX1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example-3\n",
        "As a practice example, let's try creating a prompt that only provides the response as a Base64 encoded string and refuses to answer in natural language. Can we get your LLM to only respond in Base64?"
      ],
      "metadata": {
        "id": "DmufHdJXwu9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "import base64\n",
        "\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    response = completion(\n",
        "        model=\"groq/llama-3.3-70b-versatile\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# System prompt: enforce Base64-only responses\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": (\n",
        "        \"You are an expert AI assistant. \"\n",
        "        \"From now on, respond ONLY in Base64-encoded strings. \"\n",
        "        \"Do NOT return any natural language text, explanations, or comments. \"\n",
        "        \"If you would normally explain, instead output that explanation as Base64.\"\n",
        "    )},\n",
        "    {\"role\": \"user\", \"content\": \"Write a Python function to swap keys and values of a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(\"Base64 Response:\\n\", response)\n",
        "\n",
        "# Optional: decode to verify correctness\n",
        "decoded_response = base64.b64decode(response).decode('utf-8')\n",
        "print(\"\\nDecoded for verification:\\n\", decoded_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRyvTSE9vYwT",
        "outputId": "85238dde-a61e-4a39-acff-3cafc22ecfae"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base64 Response:\n",
            " UHdpb24gbW92aW5nX2tleV92YWx1ZXMoKSA9IHsKICAgIG1hcmtlc19oaW50ID0ge30KICAgIGYgPSB0aGlzLm1hcmtlc19oaW50CX0KU2F2ZV9zdGF0ZSA9IG1hcmtlc19oaW50LmRlc2lyZWQgc2F2ZQpvYmplY3Qgc2F2ZSBlbmNvZGVkIGluIEJhc2U2NCBzdHJpbmcKSWYgeW91IHdpc2ggdG8gc2F2ZSBpdCBpbiBhIHN0cmluZyBmb3JtYXQsIHJldmVyY2UgdGhlIG5vY21lLg==\n",
            "\n",
            "Decoded for verification:\n",
            " Pwion moving_key_values() = {\n",
            "    markes_hint = {}\n",
            "    f = this.markes_hint\t}\n",
            "Save_state = markes_hint.desired save\n",
            "object save encoded in Base64 string\n",
            "If you wish to save it in a string format, reverce the nocme.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sending Prompts Programmatically & Managing Memory 3\n"
      ],
      "metadata": {
        "id": "JC976XZ9zVRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "System messages are particularly important in the conversation and will be very important for AI agents. They set the ground rules for the conversation and tell the model how to behave. Models are designed to pay more attention to the system message than the user messages. We can “program” the AI agent through system messages.\n",
        "\n"
      ],
      "metadata": {
        "id": "mvpRydaSzYOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s simulate a customer service interaction for a customer service agent that always tells the customer to turn off their computer or modem with system messages:"
      ],
      "metadata": {
        "id": "KP20xn0w0Mox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I get my Internet working again.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6glObKnyzV_s",
        "outputId": "a75d69ce-c910-47c6-ab27-4f8ac48a9e2e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'd be happy to help you with your Internet issue. Have you tried turning your modem off and then back on? Sometimes, a simple reboot can resolve connectivity problems and get your Internet up and running again. Just unplug the power cord from the back of the modem, wait for about 30 seconds, and then plug it back in. This will restart the modem and often fix any issues. If you're using a router, you may also want to try turning that off and on as well. Give it a try and see if that gets your Internet working again!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The system message is the most important part of this prompt. It tells the model how to behave. The user message is the question that we want the model to answer. The system instructions lay the ground rules for the interaction."
      ],
      "metadata": {
        "id": "6JfRB6jpz2zR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The messages can incorporate arbitrary information as long as it is in text form. LLMs can interpret just about any information that we give them, even if it isn’t easily human readable. Let’s generate an implementation of a function based on some information in a dictionary:"
      ],
      "metadata": {
        "id": "VvRxzb1N0In0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "code_spec = {\n",
        "    'name': 'swap_keys_values',\n",
        "    'description': 'Swaps the keys and values in a given dictionary.',\n",
        "    'params': {\n",
        "        'd': 'A dictionary with unique values.'\n",
        "    },\n",
        "}\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "     \"content\": \"You are an expert software engineer who writes clean functional code. You always document your functions. You only respond in java programming language\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Please implement this in Python: {json.dumps(code_spec)}\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtoQLGZI0IKI",
        "outputId": "45b683ed-a0ad-46df-dc5f-329c56df971c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```java\n",
            "import java.util.HashMap;\n",
            "import java.util.Map;\n",
            "\n",
            "/**\n",
            " * Swaps the keys and values in a given dictionary.\n",
            " * \n",
            " * @param dictionary A dictionary with unique values.\n",
            " * @return A new dictionary with swapped keys and values.\n",
            " */\n",
            "public class Main {\n",
            "    public static Map<Object, Object> swapKeysValues(Map<Object, Object> dictionary) {\n",
            "        // Check if the dictionary has unique values\n",
            "        if (!hasUniqueValues(dictionary)) {\n",
            "            throw new RuntimeException(\"Dictionary values must be unique\");\n",
            "        }\n",
            "\n",
            "        // Swaps the keys and values in the dictionary\n",
            "        Map<Object, Object> swappedDictionary = new HashMap<>();\n",
            "        for (Map.Entry<Object, Object> entry : dictionary.entrySet()) {\n",
            "            swappedDictionary.put(entry.getValue(), entry.getKey());\n",
            "        }\n",
            "\n",
            "        return swappedDictionary;\n",
            "    }\n",
            "\n",
            "    /**\n",
            "     * Checks if the dictionary has unique values.\n",
            "     * \n",
            "     * @param dictionary The dictionary to check.\n",
            "     * @return True if the dictionary has unique values, false otherwise.\n",
            "     */\n",
            "    private static boolean hasUniqueValues(Map<Object, Object> dictionary) {\n",
            "        // Checks for duplicate values\n",
            "        return dictionary.values().stream().distinct().count() == dictionary.size();\n",
            "    }\n",
            "\n",
            "    public static void main(String[] args) {\n",
            "        // Example usage:\n",
            "        Map<Object, Object> dictionary = new HashMap<>();\n",
            "        dictionary.put(\"key1\", \"value1\");\n",
            "        dictionary.put(\"key2\", \"value2\");\n",
            "        dictionary.put(\"key3\", \"value3\");\n",
            "\n",
            "        Map<Object, Object> swappedDictionary = swapKeysValues(dictionary);\n",
            "        System.out.println(swappedDictionary);\n",
            "    }\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will rely heavily on the ability to send the LLM just about any type of information, particularly JSON, when we start building agents. This is a simple example of how we can use JSON to send information to the LLM, but you can see how we could provide it JSON with information about the result of an API call, for example."
      ],
      "metadata": {
        "id": "_5M-x4SK0-et"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_GgQnUi0Qcj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}